{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST-image-classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCXpLoyayf6X"
      },
      "source": [
        "## Import relevant packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kqF_SWOxoIe"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# import data provider module\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHZzmEhZym1r"
      },
      "source": [
        "## Import MNIST dataset\n",
        "The dataset is called MNIST and refers to handwritten digit recognition.\n",
        "\n",
        "*   The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image).\n",
        "*   The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juKlsVDdyrvh"
      },
      "source": [
        "# with_info=True will also provide us with a tuple containing information about the version, features, number of samples\n",
        "# we will use this information a bit below and we will store it in mnist_info\n",
        "# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \n",
        "# alternatively, as_supervised=False, would return a dictionary\n",
        "# obviously we prefer to have our inputs and targets separated\n",
        "\n",
        "mnist_dataset, mnist_info = tfds.load(name=\"mnist\", with_info=True, as_supervised=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ8I8ChY3x0U"
      },
      "source": [
        "def scale(image, label):\n",
        "  \"\"\"Function to normalize pixel values (scale between 0 and 1)\"\"\"\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255.\n",
        "  return image, label"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT52xvnvyb1Y"
      },
      "source": [
        "# define train and test sets\n",
        "mnist_train, mnist_test = mnist_dataset[\"train\"], mnist_dataset[\"test\"]\n",
        "\n",
        "# define size of validation set (10% of train set)\n",
        "num_validation_samples = 0.1 * mnist_info.splits[\"train\"].num_examples\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
        "\n",
        "num_test_samples = 0.1 * mnist_info.splits[\"test\"].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
        "\n",
        "# normalize data\n",
        "scaled_train_and_validation_data = mnist_train.map(scale)\n",
        "test_data = mnist_test.map(scale)\n",
        "\n",
        "# set a buffer size to allow shuffling on a large dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# shuffle train data\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "# define train and validation sets\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
        "\n",
        "BATCH_SIZE = 150\n",
        "\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "validation_inputs, validation_targets = next(iter(validation_data))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyok0i2v4GlS"
      },
      "source": [
        "## Model\n",
        "### Outline the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRV2o3ux4KXz"
      },
      "source": [
        "# define layer size\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layer_size = 200\n",
        "\n",
        "# define model\n",
        "model = tf.keras.Sequential([\n",
        "                             # convert image to vector\n",
        "                             tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
        "                             # define hidden layers\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"),\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"),\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"),\n",
        "                             # define output layer\n",
        "                             tf.keras.layers.Dense(output_size, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgp8gPlr5xyG"
      },
      "source": [
        "### Choose the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riou_SiR53bc"
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFjbSnsg58od"
      },
      "source": [
        "## Training\n",
        "\n",
        "1.   At the beginning of each epoch, the training loss will be set to 0\n",
        "2.   The algorithm will iterate over a preset number of batches, all from train_data\n",
        "3.   The weights and biases will be updated as many times as there are batches\n",
        "4.   We will get a value for the loss function, indicating how the training is going\n",
        "5.   We will also see a training accuracy\n",
        "6.   At the end of the epoch, the algorithm will forward propagate the whole validation set\n",
        "\n",
        "*When we reach the maximum number of epochs, the training will be over"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6ohoVVG5-pa",
        "outputId": "399d03bc-7eae-4e8c-fd47-dc97ecbc577a"
      },
      "source": [
        "NUM_EPOCHS = 10\n",
        "\n",
        "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "360/360 - 8s - loss: 0.2939 - accuracy: 0.9144 - val_loss: 0.1466 - val_accuracy: 0.9548\n",
            "Epoch 2/10\n",
            "360/360 - 4s - loss: 0.1106 - accuracy: 0.9668 - val_loss: 0.0922 - val_accuracy: 0.9727\n",
            "Epoch 3/10\n",
            "360/360 - 4s - loss: 0.0742 - accuracy: 0.9768 - val_loss: 0.0660 - val_accuracy: 0.9797\n",
            "Epoch 4/10\n",
            "360/360 - 4s - loss: 0.0526 - accuracy: 0.9834 - val_loss: 0.0627 - val_accuracy: 0.9823\n",
            "Epoch 5/10\n",
            "360/360 - 4s - loss: 0.0402 - accuracy: 0.9877 - val_loss: 0.0496 - val_accuracy: 0.9862\n",
            "Epoch 6/10\n",
            "360/360 - 4s - loss: 0.0311 - accuracy: 0.9902 - val_loss: 0.0531 - val_accuracy: 0.9853\n",
            "Epoch 7/10\n",
            "360/360 - 4s - loss: 0.0301 - accuracy: 0.9902 - val_loss: 0.0519 - val_accuracy: 0.9867\n",
            "Epoch 8/10\n",
            "360/360 - 4s - loss: 0.0227 - accuracy: 0.9926 - val_loss: 0.0293 - val_accuracy: 0.9925\n",
            "Epoch 9/10\n",
            "360/360 - 4s - loss: 0.0218 - accuracy: 0.9931 - val_loss: 0.0743 - val_accuracy: 0.9790\n",
            "Epoch 10/10\n",
            "360/360 - 4s - loss: 0.0185 - accuracy: 0.9938 - val_loss: 0.0331 - val_accuracy: 0.9917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0276b54510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WVBEwzEBQFa"
      },
      "source": [
        "## Test the model\n",
        "After training on the training data and validating on the validation data, we test the final prediction power of our model by running it on the test dataset that the algorithm has NEVER seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9CiM_Ky8eHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72aafc73-e494-4338-c54e-c41ba4385024"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 1s 109ms/step - loss: 0.0885 - accuracy: 0.9794\n",
            "Test loss: 0.09. Test accuracy: 97.94%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_x8BM5rDCiI"
      },
      "source": [
        "We achevied an accuracy of 97.94% on the test set. We could try to play with the number of hidden layers and hyperparameters to see if the model can perform even better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3RCjqBfEuWW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}